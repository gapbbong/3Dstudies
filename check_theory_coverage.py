import re
import json

file_path = r'd:\App\3d Studies\data.js'

with open(file_path, 'r', encoding='utf-8') as f:
    content = f.read()

# Extract the appData object
json_match = re.search(r'const appData = ({.*?});', content, re.DOTALL)
if not json_match:
    print("Could not find appData")
    exit(1)

# Parse the data (we'll use a simple approach since it's JS object notation)
# For each part 3-8, extract theory and questions

parts_to_check = ['part3', 'part4', 'part5', 'part6', 'part7', 'part8']

for part_id in parts_to_check:
    print(f"\n{'='*80}")
    print(f"ë¶„ì„: {part_id.upper()}")
    print(f"{'='*80}")
    
    # Find the part section
    part_pattern = rf'"id":\s*"{part_id}".*?"theoryContent":\s*`([^`]*)`.*?"questions":\s*\[(.*?)\]\s*\}}'
    part_match = re.search(part_pattern, content, re.DOTALL)
    
    if not part_match:
        print(f"âŒ {part_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        continue
    
    theory = part_match.group(1)
    questions_section = part_match.group(2)
    
    # Extract question texts
    question_pattern = r'"question":\s*"([^"]*(?:"[^"]*"[^"]*)*)"'
    questions = re.findall(question_pattern, questions_section)
    
    print(f"\nğŸ“š í•µì‹¬ì´ë¡  í‚¤ì›Œë“œ:")
    theory_keywords = set()
    # Extract key terms from theory
    for line in theory.split('\n'):
        line = line.strip()
        if line and not line.startswith('#'):
            # Extract Korean terms and English terms
            korean_terms = re.findall(r'[ê°€-í£]{2,}', line)
            english_terms = re.findall(r'[A-Z][A-Za-z]+', line)
            theory_keywords.update(korean_terms)
            theory_keywords.update(english_terms)
    
    print(f"  ì´ {len(theory_keywords)}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
    
    print(f"\nâ“ ë¬¸ì œ ë¶„ì„ (ì´ {len(questions)}ë¬¸ì œ):")
    
    coverage_issues = []
    for i, q in enumerate(questions, 1):
        # Clean up the question text
        q_clean = q.replace('\\n', ' ').replace('\\r', '')
        
        # Extract key terms from question
        q_keywords = set()
        korean_terms = re.findall(r'[ê°€-í£]{2,}', q_clean)
        english_terms = re.findall(r'[A-Z][A-Za-z]+', q_clean)
        q_keywords.update(korean_terms)
        q_keywords.update(english_terms)
        
        # Check if question keywords are in theory
        missing = q_keywords - theory_keywords
        
        # Filter out common words
        common_words = {'ê²ƒì€', 'ëŒ€í•œ', 'ì„¤ëª…', 'ë‹¤ìŒ', 'ê²½ìš°', 'ë°©ì‹', 'í”„ë¦°í„°', 'ì¶œë ¥', 'ì œí’ˆ', 'ëª¨ë¸', 'íŒŒì¼', 'ì‘ì—…'}
        missing = missing - common_words
        
        if len(missing) > 3:  # If more than 3 specific terms are missing
            coverage_issues.append((i, q_clean[:80], list(missing)[:5]))
    
    if coverage_issues:
        print(f"\nâš ï¸  ì´ë¡ ì—ì„œ ë‹¤ë£¨ì§€ ì•Šì€ ê°œë…ì´ í¬í•¨ëœ ë¬¸ì œë“¤:")
        for q_num, q_text, missing_terms in coverage_issues:
            print(f"  Q{q_num:02d}: {q_text}...")
            print(f"       ëˆ„ë½ í‚¤ì›Œë“œ: {', '.join(missing_terms)}")
    else:
        print(f"\nâœ… ëª¨ë“  ë¬¸ì œê°€ í•µì‹¬ì´ë¡ ìœ¼ë¡œ ì»¤ë²„ë©ë‹ˆë‹¤!")

print(f"\n{'='*80}")
print("ë¶„ì„ ì™„ë£Œ")
print(f"{'='*80}")
